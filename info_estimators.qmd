---
title: "Estimators"
bibliography: references.bib
nocite: |
  @diaz2023nonparametric, @hoffman2023introducing, @kennedy2019nonparametric @haneuse2013estimation, @bickel1993efficient
---

{{< include macros.qmd >}}

To recap, we've defined a causal parameter $\theta$ that is a function of a general hypothetical intervention, $\dd_t(a_t, h_t, \epsilon)$ and we've laid out some assumptions to identify this parameter from observed data. It's now time to construct estimators.

### Sequential Regression Estimator

::: {.callout-note icon="false"}
## Algorithm: Sequential regressions

1.  Set $\m_{i,\tau +1} = Y_i$.

    For $t = \tau, ..., 1$:

    1.  Using a pre-specified parametric model, regress $\m_{i,\tau +1}$ on $\{A_{i, t}, H_{i,t}\}$.

    2.  Generate predictions from this model with $A_{i,t}$ changed to $A^{\dd}_{i,t}$. Set $\m_{i, t}$ as these predictions.

2.  Take the final estimate as $\hat{\theta} = \frac{1}{n}\sum_{i=1}^n \m_{i, 1}$.

3.  Compute standard errors using a bootstrap of steps 1 and 2.
:::

### Density-ratio estimation {#sec-density-ratio-estimation}

The following three estimators all rely on estimating the density ratio $r_t(a_t, h_t) = \frac{\g_t^\dd(a_t \mid h_t)}{\g_t(a_t \mid h_t)}$. We will often refer to this ratio as the *intervention mechanism* throughout the workshop.

We can directly estimate this density ratio with a classification trick. This process is fully automated and hidden from the user. Specifically, the IPW, TMLE, and SDR estimation methods require estimation of the ratio of the densities of $A_t^\dd$ and $A_t$, conditional on the history $H_t$, defined as $r_t$ above. This is achieved through computing the odds in a classification problem in an augmented dataset with $2n$ observations where the outcome is the auxiliary variable $\Lambda$ (defined below) and the predictors are the variables $A_t$ and $H_t$. In the $2n$ augmented data set, the data structure at time $t$ is redefined as

$$
(H_{\lambda, i, t}, A_{\lambda, i, t}, \Lambda_{\lambda, i} : \lambda = 0, 1; i = 1, ..., n)
$$

where $\Lambda_{\lambda, i} = \lambda_i$ indexes duplicate values. For all duplicated observations $\lambda\in\{0,1\}$ with the same $i$, $H_{\lambda, i, t}$ is the same. For $\lambda = 0$, $A_{\lambda, i, t}$ equals the observed exposure values $A_{i, t}$, whereas for $\lambda=1$, $A_{\lambda, i, t}$ equals the exposure values under the intervention $\dd$, namely $A^{\dd}_t$. The classification approach to density ratio estimation proceeds by estimating the conditional probability that $\Delta=1$ in this dataset, and dividing it by the corresponding estimate of the conditional probability that $\Delta=0$. Specifically, denoting $p^\lambda$ the distribution of the data in the augmented dataset, we have:

$$
r_t(a_t, h_t) = \frac{p^\lambda(a_t, h_t \mid \Lambda =
    1)}{p^\lambda(a_t, h_t \mid \Lambda =
    0)}=\frac{p^\lambda(\Lambda = 1\mid A_t=a_t,
    H_t=h_t)}{p^\lambda(\Lambda = 0\mid A_t=a_t, H_t=h_t)}
$$

### Inverse Probability Weighting

$$
\theta = \E \bigg[ \bigg\{\prod_{t=1}^\tau r_t(a_t, h_t) \bigg\} Y \bigg]
$$

::: {.callout-note icon="false"}
## Algorithm: IPW Estimator

1.  Construct estimates of $r_{i,t}(a_t, h_t)$ using the density ratio classification trick and a pre-specified parametric model.

2.  Define the weights $w_{i,t} = \prod_{t=1}^\tau r_{i,t}(a_t, h_t)$.

3.  Take the final estimate as $\hat{\theta} = \frac{1}{n}\sum_{i=1}^n w_{i,t}\times y_i$.

4.  Compute standard errors using a bootstrap of steps 1-3.
:::

### Doubly Robust Estimators

The sequential regressions and IPW estimators are parametric estimators. We will now turn our attention to two non-parametric estimators. Key to constructing these estimators is the *efficient influence function* (EIF). The EIF characterizes the asymptotic behavior of all regular and efficient estimators.

::: callout-important
## Assumptions

1.  The treatment $A$ is discrete, or

2.  If $A$ is continuous, the function $\dd$ is piecewise smooth invertible

3.  The function $\dd$ does not depend on the observed distribution $\P$
:::

INSERT HERE THE DEFINITION OF PIECEWISE SMOOTH INVERTIBILITY

Taken together, these assumptions ensure that the efficient influence function of $\theta$ for interventions $\dd$ have a structure similar to the influence function for the effect of dynamic regimes. This allows for multiply robust estimation, which is not generally possible for interventions $\dd$ that depend on $\P$.

Define the function

$$
\phi_t: o \mapsto \sum_{s=t}^\tau \bigg( \prod_{k=t}^s r_k(a_k, h_k)\bigg) \big\{\m_{s+1}(a_{s+1}^\dd, h_{s+1}) - \m_s(a_s, h_s) \big\} + \m_t(a_t^\dd, h_t).
$$

The efficient influence function for estimating $\theta = \E[\m_1(A^\dd, L_1)]$ in the non-parametric model is given by $\phi_1(O) - \theta$. In the case of single time-point, the influence function simplifies to

$$
r(a, w)\{Y - \m(a,w)\} + \m(a^{\dd},w) - \theta.
$$

#### Targeted Minimum-Loss Based Estimation

::: {.callout-note icon="false"}
## Algorithm: TMLE
:::

#### Sequentially Doubly Robust Estimator

::: {.callout-note icon="false"}
## Algorithm: SDR Estimator
:::

### Choosing an Estimator

How should we choose which estimator to use? In general we never recommend using the IPW or sequential regression estimator. Both require the use of correctly pre-specified parametric models for valid statistical inference (ya right üòÇ). The TMLE and SDR estimators, however, are both doubly or sequentially doubly robust and can be used with machine-learning algorithms while remaining $\sqrt{n}$-consistent.

Wait, what does it mean for an estimator to be doubly robust? For the simple case of a single time point, an estimator is considered doubly robust if it is able to produce a consistent estimate of the target parameter as long as at least one of the nuisance parameters is consistently estimated. For time-varying setting, an estimator is doubly robust if, for some time $s$, all outcome regressions for $t >s$ are consistently estimated and all intervention mechanisms for $t \leq s$ are consistently estimated. Sequential double robustness (often also referred to as $2^\tau$-multiply robust) implies that an estimator is consistent if for all times either the outcome or intervention mechanism is consistently estimated.

While the SDR estimator may be more robust to model misspecification, the TMLE does have the advantage of being a substitution estimator. Because of this, estimates from the TMLE are guaranteed to stay within the valid range of the outcome. Taken together, this leads to the following recommendations for choosing between the TMLE and SDR:

::: callout-tip
-   If treatment is not time-varying, use the TMLE.

-   If treatment is time-varying first try the SDR.

-   If the SDR estimator produces "out-of-bounds" estimates, instead use the TMLE.
:::

|                                       | IPW | Sequential regression | TMLE | SDR |
|---------------------------------------|:---:|:---------------------:|:----:|:---:|
| Uses outcome regression               |     |          ‚≠ê           |  ‚≠ê  | ‚≠ê  |
| Uses the propensity score             | ‚≠ê  |                       |  ‚≠ê  | ‚≠ê  |
| Valid inference with machine-learning |     |                       |  ‚≠ê  | ‚≠ê  |
| Substitution estimator                |     |          ‚≠ê           |  ‚≠ê  |     |
| Doubly robust                         |     |                       |  ‚≠ê  | ‚≠ê  |
| Sequentially doubly robust            |     |                       |      | ‚≠ê  |

: **Table 1.** Properties of estimators.

### Crossfitting

blah blah blah
